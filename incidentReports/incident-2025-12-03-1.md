**SUMMARY**:

Between the hour of 8:15 and 8:49AM on 12/03/2025, all simulated traffic interacting with the JWT Pizza Service experienced severe network instability, connection resets, and timeouts.

The event was triggered by a chaos engineering test executed at 08:15, which intentionally disrupted outbound network traffic from the service. The injected chaos caused outbound HTTPS requests (used for metrics shipping to Grafana Cloud) to begin failing with UND_ERR_CONNECT_TIMEOUT and ECONNRESET.

A bug in the service’s metrics-sending code path caused errors to stack rapidly when the external endpoint became unreachable. Instead of degrading gracefully, the service repeatedly attempted to send metrics, resulting in persistent failures and saturation of the logging system.

The incident was detected through Grafana Loki log volume spikes, and later confirmed by failed outbound requests in the Node.js runtime. The system remained operational, and pizza-ordering functionality stayed responsive, but telemetry and observability were degraded. This SEV-2 incident affected 100% of simulated users, since all traffic triggered metrics-sending behavior.

**DETECTION**:

The incident was detected alerts were sent to Lacy Miller firing users active alerts and high frequency pizza requests at the same time when Grafana Loki showed a sudden spike of:

ECONNRESET

UND_ERR_CONNECT_TIMEOUT

“Failed to send metric” errors

At 08:18 AM, error logs began appearing at a rate significantly above baseline.

Improvements planned:

Add an additional alert specifically for metric ingestion failure rate, to distinguish between service downtime and purely telemetry degradation.

Add a system health endpoint alert, to cross-check service responsiveness during chaos events.

**IMPACT**:

For 34 minutes, between 08:15 AM and 08:49 AM, the system experienced continuous failures in sending outbound metrics, affecting:

100% of simulated traffic

All attempts to send metrics such as:

http_requests_total

revenue_total

pizza_failed

request_latency

Users were still able to place orders locally, and the API remained responsive, but observability tools did not receive accurate telemetry during the incident.

No support tickets were generated (class simulation), but all traffic logs reflected the degraded state.

**TIMELINE**:

08:15 — Chaos test begins, disrupting outbound network connectivity.

08:17 — First errors appear: UND_ERR_CONNECT_TIMEOUT.

08:18 — Node.js logs repeatedly show ECONNRESET and “Failed to send metric.”

08:22 — Log volume in Grafana spikes sharply; Loki shows hundreds of failures.

08:29 — All simulated traffic continues failing telemetry uploads; system otherwise operational.

08:35 — Investigation begins; chaos test is still running during traffic simulation.

08:42 — Chaos experiment ends; outbound connections begin recovering.

08:49 — All metrics resume sending successfully; log errors taper back to normal.

**RESPONSE**:

After receiving a page at {XX:XX UTC}, {ON-CALL ENGINEER} came online at {XX:XX UTC} in {SYSTEM WHERE INCIDENT INFO IS CAPTURED}.

This engineer did not have a background in the {AFFECTED SYSTEM} so a second alert was sent at {XX:XX UTC} to {ESCALATIONS ON-CALL ENGINEER} into the who came into the room at {XX:XX UTC}.

**ROOT CAUSE**:

Chaos engineering test intentionally broke outbound HTTPS connectivity.

A bug in the metrics connection-pool handler caused connections to leak under failure conditions.

The metrics sender lacked:

Retry backoff

Circuit breaking

Graceful degradation logic

Excessive retries saturated the logging system, further obscuring signal.

This root cause has parallels with prior failures (HOT-13432, HOT-14932, HOT-19452), suggesting a pattern of instability in the metrics pipeline under degraded network conditions.

**RESOLUTION**:
The following steps successfully restored service functionality:

Increased the size of the BuildEng EC3 ASG, ensuring enough nodes were available to handle workload spikes and prevent scheduling issues on overloaded nodes.

Disabled the Escalator autoscaler, which had been aggressively deallocating resources and contributing to system churn.

Reverted the Build Engineering scheduler to the previous stable version, eliminating connection leaks observed in the newer build.

Metrics resumed delivery at 08:49 AM, and logging throughput returned to baseline.

**PREVENTION**:

To avoid recurrence, the following improvements are planned:

Add retry backoff and circuit-breaker logic to the metrics delivery subsystem.

Instrument connection-pool health metrics, including active, idle, and failed connections.

Add integration tests simulating outbound-network failures.

Improve alerting by separating:

API availability alerts

Metrics ingestion alerts

System resource saturation alerts

Improve deployment testing to ensure scheduler changes do not introduce connection leaks.

**ACTION ITEMS**:

Temporary manual auto-scaling rate limit to minimize node churn.

Re-introduce job rate limiting after full unit-test coverage is achieved.

Deploy a secondary distributed rate-tracking system to improve scaling behavior.

Add graceful retry logic to metrics sender to prevent log flooding.

Implement circuit breakers around external telemetry calls.

Conduct a post-chaos audit to ensure subsystems behave predictably during intentional failures.